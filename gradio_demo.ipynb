{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram model using empirical frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "\n",
    "def character_tokenizer(names_str):\n",
    "    vocab = sorted(list(set(names_str)) + [\"<S>\"])\n",
    "    s_to_i = {s: i for i, s in enumerate(vocab)}\n",
    "    i_to_s = {i: s for i, s in enumerate(vocab)}\n",
    "    return s_to_i, i_to_s\n",
    "\n",
    "\n",
    "def tokenize_name(name, s_to_i):\n",
    "    token_sequence = [s_to_i[\"<S>\"]]  # special token for start of name\n",
    "    for character in name:\n",
    "        token_sequence.append(s_to_i[character])\n",
    "    token_sequence.append(s_to_i[\"<S>\"])  # special token for end of name\n",
    "    return token_sequence\n",
    "\n",
    "\n",
    "def visualize_bigram_counts_frequencies(i_to_s, counts_matrix, freqs_matrix):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(freqs_matrix, interpolation=\"nearest\", cmap=\"Blues\")\n",
    "\n",
    "    # add the bigram and its count inside each cell\n",
    "    for i, _ in enumerate(counts_matrix):\n",
    "        for j, _ in enumerate(counts_matrix[i]):\n",
    "            ax.text(\n",
    "                j,\n",
    "                i,\n",
    "                (\n",
    "                    f\"Bigram = {i_to_s[i]} → {i_to_s[j]}\\n\"\n",
    "                    f\"count = {counts_matrix[i][j]:.2f}\\n\"\n",
    "                    f\"freq = {freqs_matrix[i][j]:.2f}\\n\"\n",
    "                ),\n",
    "                ha=\"center\",\n",
    "                va=\"top\",\n",
    "                color=\"black\",\n",
    "                fontsize=8,\n",
    "            )\n",
    "\n",
    "    # keep ticks as integer tokens\n",
    "    ax.set_xticks(range(len(i_to_s)))\n",
    "    ax.set_yticks(range(len(i_to_s)))\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def sample_name_from_prob_dist(prob_dist, s_to_i, i_to_s):\n",
    "    # start with an empty name\n",
    "    name = \"\"\n",
    "\n",
    "    # current token is initialized to <S> special token for start of name\n",
    "    token = s_to_i[\"<S>\"]\n",
    "    while True:\n",
    "        # return the probability of next token given the current token\n",
    "        prob_next_token = prob_dist[token]\n",
    "\n",
    "        # given a probability distribution return an integer token sampled from it\n",
    "        next_token = torch.multinomial(input=prob_next_token, num_samples=1).item()\n",
    "\n",
    "        # convert the integer token to a character\n",
    "        next_char = i_to_s[next_token]\n",
    "\n",
    "        # if we have reached the <S> special token for end of name, stop\n",
    "        if next_char == \"<S>\":\n",
    "            break\n",
    "\n",
    "        # update the name\n",
    "        name += next_char\n",
    "\n",
    "        # update the current token\n",
    "        token = next_token\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "def bigram_output(names_str, n_names_to_sample=10, seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # -------------------- Character-level Tokenization --------------------\n",
    "    s_to_i, i_to_s = character_tokenizer(names_str.replace(\"\\n\", \"\"))\n",
    "\n",
    "    # -------------------- Tokenized Names --------------------\n",
    "    tokenized_names = [tokenize_name(name, s_to_i) for name in names_str.split(\"\\n\")]\n",
    "\n",
    "    # -------------------- Bigram Prediction Task Examples --------------------\n",
    "    bigram_prediction_tasks = [\n",
    "        f\"Given {tokenized_char} ({i_to_s[tokenized_char]}) predict {next_tokenized_char} ({i_to_s[next_tokenized_char]})\"\n",
    "        for tokenized_name in tokenized_names\n",
    "        for (tokenized_char, next_tokenized_char) in zip(\n",
    "            tokenized_name[:-1], tokenized_name[1:]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # -------------------- Observed counts and frequencies --------------------\n",
    "    counts = Counter(\n",
    "        [\n",
    "            (tokenized_char, next_tokenized_char)\n",
    "            for tokenized_name in tokenized_names\n",
    "            for tokenized_char, next_tokenized_char in zip(\n",
    "                tokenized_name[:-1], tokenized_name[1:]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # produce a matrix of the counts of size (vocab_size, vocab_size)\n",
    "    counts_matrix = torch.Tensor([[counts[(i, j)] for j in i_to_s] for i in i_to_s])\n",
    "\n",
    "    # produce a matrix of the frequencies by normalizing the counts matrix\n",
    "    freqs_matrix = counts_matrix / counts_matrix.sum(dim=1, keepdim=True)\n",
    "\n",
    "    # produce an plot of the counts and frequencies matrices\n",
    "    fig = visualize_bigram_counts_frequencies(i_to_s, counts_matrix, freqs_matrix)\n",
    "\n",
    "    # -------------------- Sampled Output --------------------\n",
    "    sampled_names = []\n",
    "    for _ in range(n_names_to_sample):\n",
    "        name = sample_name_from_prob_dist(freqs_matrix, s_to_i, i_to_s)\n",
    "        sampled_names.append(name)\n",
    "\n",
    "    # -------------------- Random Sampled Output --------------------\n",
    "    prob_uniform = torch.ones_like(freqs_matrix)\n",
    "    prob_uniform /= prob_uniform.sum(dim=1, keepdim=True)\n",
    "    random_sampled_names = []\n",
    "    for _ in range(n_names_to_sample):\n",
    "        name = sample_name_from_prob_dist(prob_uniform, s_to_i, i_to_s)\n",
    "        random_sampled_names.append(name)\n",
    "\n",
    "    return (\n",
    "        s_to_i,\n",
    "        \"\\n\".join(\n",
    "            [\n",
    "                f\"{name} → {tokenized_name}\"\n",
    "                for name, tokenized_name in zip(names, tokenized_names)\n",
    "            ]\n",
    "        ),\n",
    "        \"\\n\".join(bigram_prediction_tasks),\n",
    "        fig,\n",
    "        \"\\n\".join(sampled_names),\n",
    "        \"\\n\".join(random_sampled_names),\n",
    "    )\n",
    "\n",
    "\n",
    "names = [\"emma\", \"ava\"]\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            title = gr.Markdown(\n",
    "                \"\"\"\n",
    "                # Bigram Language Model using empirical frequencies\n",
    "                Shows how to build a simple probabilistic language model using bi-grams.\n",
    "                \"\"\"\n",
    "            )\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            text1 = gr.Textbox(label=\"Names Dataset\", value=\"\\n\".join(names))\n",
    "            inbtw = gr.Button(\"Submit\", variant=\"primary\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out1 = gr.Textbox(label=\"Character-level Tokenization\")\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out2 = gr.Textbox(label=\"Tokenized Names\")\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out3 = gr.Textbox(label=\"Bigram Prediction Task Examples\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            out4 = gr.Plot(label=\"Observed counts and frequencies\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out5 = gr.Textbox(label=\"Sampled Output\")\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out6 = gr.Textbox(label=\"Random Sampled Output\")\n",
    "\n",
    "    inbtw.click(\n",
    "        bigram_output, inputs=text1, outputs=[out1, out2, out3, out4, out5, out6]\n",
    "    )\n",
    "\n",
    "demo.launch(quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we evaluate our bigram model? Negative log likelihood\n",
    "\n",
    "\n",
    "Take a dataset $\\text{data}$ we want to evaluate our model against\n",
    "\n",
    "An ideal model would have a $$P(\\text{data}) = 1$$\n",
    "\n",
    "In plain english, this means that the model would be able to predict all the characters in the dataset with 100% certainty.\n",
    "\n",
    "Given data is a sequence of characters, we can calculate the probability of the dataset as follows\n",
    "\n",
    "$$P(\\text{data}) = \\prod_{i=1}^{n} P(\\text{data}_i | \\text{data}_{i-1})$$ \n",
    "\n",
    "i.e. a joint probability of all the characters in the dataset\n",
    "\n",
    "where $n$ is the length of the dataset and $P(\\text{data}_i | \\text{data}_{i-1})$ is the conditional probability of the ith character in the dataset given the previous characters (in the case of bigram only the most previous character).\n",
    "\n",
    "We chose to apply a log transform to the probability to avoid underflow errors given we are multiplying probabilities between 0 and 1. This is acceptable given that the log function is monotonically increasing between 0 and 1. We call this our log-likelihood:\n",
    "\n",
    "$$Log(P(\\text{data})) = Log(\\prod_{i=1}^{n} P(\\text{data}_i | \\text{data}_{i-1}))$$\n",
    "\n",
    "And given  $Log(a * b) = log(a) + log(b)$ we can further simplify the log-likehood as follows\n",
    "\n",
    "$$ Log(P(\\text{data})) = \\sum_{i=1}^{n} Log(P(\\text{data}_i | \\text{data}_{i-1}))$$\n",
    "\n",
    "We then normalize the score by taking a mean and as such we compute a `mean_log_likelihood`\n",
    "\n",
    "$$\\bar{ll} = \\frac{1}{n} *  \\sum_{i=1}^{n} Log(P(\\text{data}_i | \\text{data}_{i-1}))$$\n",
    "\n",
    "Note that in order to frame this as a minimization problem, we take the negative of the log-likelihood as the loss value to minimize.\n",
    "\n",
    "$$\\bar{nll} = \\frac{-1}{n} *  \\sum_{i=1}^{n} Log(P(\\text{data}_i | \\text{data}_{i-1}))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7878\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7878/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "\n",
    "def character_tokenizer(names_str):\n",
    "    vocab = sorted(list(set(names_str)) + [\"<S>\"])\n",
    "    s_to_i = {s: i for i, s in enumerate(vocab)}\n",
    "    i_to_s = {i: s for i, s in enumerate(vocab)}\n",
    "    return s_to_i, i_to_s\n",
    "\n",
    "\n",
    "def tokenize_name(name, s_to_i):\n",
    "    token_sequence = [s_to_i[\"<S>\"]]  # special token for start of name\n",
    "    for character in name:\n",
    "        token_sequence.append(s_to_i[character])\n",
    "    token_sequence.append(s_to_i[\"<S>\"])  # special token for end of name\n",
    "    return token_sequence\n",
    "\n",
    "\n",
    "def bigram_evaluation(training_dataset, evaluation_dataset, apply_smoothing):\n",
    "    # create a character-level tokenizer\n",
    "    s_to_i, i_to_s = character_tokenizer(training_dataset.replace(\"\\n\", \"\"))\n",
    "    vocab_size = len(s_to_i)\n",
    "\n",
    "    # tokenize each name in the dataset into a sequence of integers\n",
    "    tokenized_names = [\n",
    "        tokenize_name(name, s_to_i) for name in training_dataset.split(\"\\n\")\n",
    "    ]\n",
    "\n",
    "    # build counts of each bigram\n",
    "    counts = Counter(\n",
    "        [\n",
    "            (tokenized_char, next_tokenized_char)\n",
    "            for tokenized_name in tokenized_names\n",
    "            for tokenized_char, next_tokenized_char in zip(\n",
    "                tokenized_name[:-1], tokenized_name[1:]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # produce a matrix of the counts\n",
    "    counts_matrix = torch.Tensor([[counts[(i, j)] for j in i_to_s] for i in i_to_s])\n",
    "\n",
    "    # ---------------------------- Apply Smoothing ----------------------------\n",
    "    if apply_smoothing:\n",
    "        # ensure that no count is zero\n",
    "        counts_matrix += 1\n",
    "\n",
    "    # produce a matrix of the frequencies\n",
    "    freqs_matrix = counts_matrix / counts_matrix.sum(dim=1, keepdim=True)\n",
    "\n",
    "    # ---------------------------- Evaluate Model ----------------------------\n",
    "\n",
    "    # ---------------------------- Mean Negative Log Likelihood - Bigram Model ----------------------------\n",
    "    log_likelihood = 0\n",
    "    n = 0\n",
    "    for name in evaluation_dataset.split(\"\\n\"):\n",
    "        tokenized_name = tokenize_name(name, s_to_i)\n",
    "\n",
    "        # for each name we always start with the special token for start of name\n",
    "        token = 0\n",
    "        for next_token in tokenized_name[1:]:\n",
    "            # take the estimated probability of the observed next token\n",
    "            prob_next_token = freqs_matrix[token][next_token]\n",
    "\n",
    "            # transform to log space\n",
    "            log_prob_next_token = torch.log(prob_next_token)\n",
    "\n",
    "            # add to the log likelihood\n",
    "            log_likelihood += log_prob_next_token\n",
    "\n",
    "            # keep track of the number of tokens\n",
    "            n += 1\n",
    "\n",
    "            # update the current token\n",
    "            token = next_token\n",
    "\n",
    "    # compute the mean negative log likelihood\n",
    "    negative_mean_log_likelihood = -1 / n * log_likelihood\n",
    "\n",
    "    # ---------------------------- Mean Negative Log Likelihood - Random Model ----------------------------\n",
    "    # This is summarized as -1 / n * (n * log(1 / vocab_size)) = -1 * log(1 / vocab_size)\n",
    "    random_negative_mean_log_likelihood = -1 * torch.log(torch.tensor(1 / vocab_size))\n",
    "\n",
    "    return (\n",
    "        f\"{negative_mean_log_likelihood.item():.4f}\",\n",
    "        f\"{random_negative_mean_log_likelihood.item():.4f}\",\n",
    "    )\n",
    "\n",
    "\n",
    "training_dataset = [\n",
    "    \"emma\",\n",
    "    \"evan\",\n",
    "    \"ava\",\n",
    "]\n",
    "\n",
    "evaluation_dataset = [\"eva\"]\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            title = gr.Markdown(\n",
    "                \"\"\"\n",
    "                # Bigram Model Language Model Evaluation - Log Likelihood\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            text1 = gr.Textbox(\n",
    "                label=\"Training Dataset\", value=\"\\n\".join(training_dataset)\n",
    "            )\n",
    "            text2 = gr.Textbox(\n",
    "                label=\"Evaluation Dataset\", value=\"\\n\".join(evaluation_dataset)\n",
    "            )\n",
    "            checkbox = gr.Checkbox(label=\"Apply Smoothing\")\n",
    "            inbtw = gr.Button(\"Evaluate Model\", variant=\"primary\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out1 = gr.Textbox(label=\"Mean Negative Log Likelihood - Bigram Model\")\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out2 = gr.Textbox(label=\"Mean Negative Log Likelihood - Random Model\")\n",
    "\n",
    "    inbtw.click(\n",
    "        bigram_evaluation, inputs=[text1, text2, checkbox], outputs=[out1, out2]\n",
    "    )\n",
    "\n",
    "demo.launch(quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram model using a linear model built as a neural network minimizing log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7879\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7879/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "def character_tokenizer(names_str):\n",
    "    vocab = sorted(list(set(names_str)) + [\"<S>\"])\n",
    "    s_to_i = {s: i for i, s in enumerate(vocab)}\n",
    "    i_to_s = {i: s for i, s in enumerate(vocab)}\n",
    "    return s_to_i, i_to_s\n",
    "\n",
    "\n",
    "def tokenize_name(name, s_to_i):\n",
    "    token_sequence = [s_to_i[\"<S>\"]]  # special token for start of name\n",
    "    for character in name:\n",
    "        token_sequence.append(s_to_i[character])\n",
    "    token_sequence.append(s_to_i[\"<S>\"])  # special token for end of name\n",
    "    return token_sequence\n",
    "\n",
    "\n",
    "def sample_name_from_prob_dist(prob_dist, s_to_i, i_to_s):\n",
    "    # start with an empty name\n",
    "    name = \"\"\n",
    "\n",
    "    # current token is initialized to <S> special token for start of name\n",
    "    token = s_to_i[\"<S>\"]\n",
    "    while True:\n",
    "        # return the probability of next token given the current token\n",
    "        prob_next_token = prob_dist[token]\n",
    "\n",
    "        # given a probability distribution return an integer token sampled from it\n",
    "        next_token = torch.multinomial(input=prob_next_token, num_samples=1).item()\n",
    "\n",
    "        # convert the integer token to a character\n",
    "        next_char = i_to_s[next_token]\n",
    "\n",
    "        # if we have reached the <S> special token for end of name, stop\n",
    "        if next_char == \"<S>\":\n",
    "            break\n",
    "\n",
    "        # update the name\n",
    "        name += next_char\n",
    "\n",
    "        # update the current token\n",
    "        token = next_token\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.W = torch.randn(input_size, output_size) * 0.01\n",
    "        self.W.requires_grad_()\n",
    "\n",
    "    def forward(self, X):\n",
    "        # B, I @ I, O -> B, O\n",
    "        return X @ self.W\n",
    "\n",
    "\n",
    "def fit_model(model, X_one_hot, Y):\n",
    "    for _ in range(10_000):\n",
    "        # we call these logits\n",
    "        log_counts = model.forward(X_one_hot.float())\n",
    "\n",
    "        # perform a softmax\n",
    "        counts = torch.exp(log_counts)\n",
    "        probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "\n",
    "        # negative mean log likelihood loss\n",
    "        loss = (\n",
    "            -1 * torch.log(probs[torch.arange(Y.shape[0]), Y.long()]).sum() / Y.shape[0]\n",
    "        )\n",
    "\n",
    "        model.W.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        model.W.data -= 0.1 * model.W.grad\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def predict_proba(model, X):\n",
    "    logits = model.forward(X)\n",
    "    counts = torch.exp(logits)\n",
    "    return counts / counts.sum(dim=1, keepdim=True)\n",
    "\n",
    "\n",
    "def plot_prob_dist(probs, i_to_s):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(probs, interpolation=\"nearest\", cmap=\"Blues\")\n",
    "    for i in range(len(probs)):\n",
    "        for j in range(len(probs[i])):\n",
    "            ax.text(\n",
    "                j,\n",
    "                i,\n",
    "                (f\"Bigram = {i_to_s[i]} → {i_to_s[j]} \\n prob = {probs[i][j]:.2f}\"),\n",
    "                ha=\"center\",\n",
    "                va=\"top\",\n",
    "                color=\"black\",\n",
    "                fontsize=8,\n",
    "            )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_prob_dist(model, vocab_size, i_to_s):\n",
    "    X_tokens = torch.arange(vocab_size)\n",
    "    X_one_hot = torch.nn.functional.one_hot(input=X_tokens, num_classes=vocab_size)\n",
    "    prob_dist = predict_proba(model, X_one_hot.float())\n",
    "    return prob_dist\n",
    "\n",
    "\n",
    "def bigram_via_linear_model(names_str, n_names_to_sample=10, seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # -------------------- Character-level Tokenization --------------------\n",
    "    s_to_i, i_to_s = character_tokenizer(names_str.replace(\"\\n\", \"\"))\n",
    "    vocab_size = len(s_to_i)\n",
    "\n",
    "    # -------------------- Tokenized Names --------------------\n",
    "    tokenized_names = [tokenize_name(name, s_to_i) for name in names_str.split(\"\\n\")]\n",
    "\n",
    "    # -------------------- Model input (X) --------------------\n",
    "    xs = [token for tokenized_name in tokenized_names for token in tokenized_name[:-1]]\n",
    "    X = torch.tensor(xs)\n",
    "\n",
    "    # -------------------- Model targets (Y) --------------------\n",
    "    ys = [token for tokenized_name in tokenized_names for token in tokenized_name[1:]]\n",
    "    Y = torch.Tensor(ys)\n",
    "\n",
    "    # -------------------- One-hot encoded input --------------------\n",
    "    X_one_hot = torch.nn.functional.one_hot(input=X, num_classes=vocab_size)\n",
    "    fig_one_hot, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(X_one_hot, interpolation=\"nearest\", cmap=\"Greys\")\n",
    "\n",
    "    # -------------------- Linear Model --------------------\n",
    "    model = Linear(input_size=vocab_size, output_size=vocab_size)\n",
    "\n",
    "    # -------------------- Training Loss --------------------\n",
    "    loss = fit_model(model, X_one_hot, Y)\n",
    "\n",
    "    # -------------------- Model-Fitted Probabilities --------------------\n",
    "    prob_dist = get_prob_dist(model, vocab_size, i_to_s)\n",
    "    fig_probs = plot_prob_dist(prob_dist, i_to_s)\n",
    "\n",
    "    # -------------------- Sampled Output --------------------\n",
    "    sampled_names = []\n",
    "    for _ in range(n_names_to_sample):\n",
    "        name = sample_name_from_prob_dist(prob_dist, s_to_i, i_to_s)\n",
    "        sampled_names.append(name)\n",
    "\n",
    "    # -------------------- Random Sampled Output --------------------\n",
    "    prob_uniform = torch.ones_like(prob_dist)\n",
    "    prob_uniform /= prob_uniform.sum(dim=1, keepdim=True)\n",
    "    random_sampled_names = []\n",
    "    for _ in range(n_names_to_sample):\n",
    "        name = sample_name_from_prob_dist(prob_uniform, s_to_i, i_to_s)\n",
    "        random_sampled_names.append(name)\n",
    "\n",
    "    return (\n",
    "        s_to_i,\n",
    "        \"\\n\".join(\n",
    "            [\n",
    "                f\"{name} → {tokenized_name}\"\n",
    "                for name, tokenized_name in zip(names, tokenized_names)\n",
    "            ]\n",
    "        ),\n",
    "        xs,\n",
    "        ys,\n",
    "        fig_one_hot,\n",
    "        f\"{loss.item():.4f}\",\n",
    "        fig_probs,\n",
    "        \"\\n\".join(sampled_names),\n",
    "        \"\\n\".join(random_sampled_names),\n",
    "    )\n",
    "\n",
    "\n",
    "names = [\n",
    "    \"emma\",\n",
    "    \"ava\",\n",
    "]\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            title = gr.Markdown(\n",
    "                \"\"\"\n",
    "                # Bigram Language Model via Linear Model\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            text1 = gr.Textbox(label=\"Names Dataset\", value=\"\\n\".join(names))\n",
    "            inbtw = gr.Button(\"Submit\", variant=\"primary\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out1 = gr.Textbox(label=\"Character-level Tokenization\")\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out2 = gr.Textbox(label=\"Tokenized Input\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out3 = gr.Textbox(label=\"Model input (X)\")\n",
    "            out4 = gr.Textbox(label=\"Model targets (Y)\")\n",
    "\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out5 = gr.Plot(label=\"One-hot encoded input\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            out6 = gr.Textbox(label=\"Training Loss (Negative Mean Log Likelihood)\")\n",
    "            out7 = gr.Plot(label=\"Model-Fitted Probabilities\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out8 = gr.Textbox(label=\"Sampled Output\")\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out9 = gr.Textbox(label=\"Random Sampled Output\")\n",
    "\n",
    "    inbtw.click(\n",
    "        bigram_via_linear_model,\n",
    "        inputs=text1,\n",
    "        outputs=[out1, out2, out3, out4, out5, out6, out7, out8, out9],\n",
    "    )\n",
    "\n",
    "demo.launch(quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Curse of Dimensionality - Embeddings to the Rescue\n",
    "\n",
    "**Exponential Growth in dimensions**\n",
    "If we wanted to extend our content length to use a sequence of two characters instead of just one to predict the next character then our probability distribution matrix will now be `vocab_size` x `vocab_size` to cover all possible combinations of two characters. If we take this out to using three characters, then we will now need `vocab_size^3` (an expontentail growth in the number of rows of our matrix size)  - this means we quickly will:\n",
    "\n",
    "- have a very large matrix that is very sparse (i.e. most of the values will be zero)\n",
    "- need a lot of data to ensure we have enough examples of each combination of the characters. \n",
    "\n",
    "\n",
    "**Embedding into a vector space**\n",
    "Instead, what Bengio et al. (2003) proposed was to embed each character into a vector space of a fixed size that is smaller than the vocabulary size. By compressing each character into a vector space, we can\"\n",
    "\n",
    "- learn the relationships between characters enabling us to transfer knowledge between similar characters.\n",
    "- reduce the amount of data required to train the model.\n",
    "\n",
    "In the paper, Bengio et al. (2003) used a 30-dimensional vector space to embed each word from a vocab size of 17,000. They then provide the following example of how the embedding can be used to learn the relationship between words:\n",
    "\n",
    "\"The cat is walking in the bedroom\" can be used to generalize to \"A dog was running in a room\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7875\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7875/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from collections import deque\n",
    "\n",
    "import gradio as gr\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "def character_tokenizer(names_str):\n",
    "    vocab = sorted(list(set(names_str)) + [\"<S>\"])\n",
    "    s_to_i = {s: i for i, s in enumerate(vocab)}\n",
    "    i_to_s = {i: s for i, s in enumerate(vocab)}\n",
    "    return s_to_i, i_to_s\n",
    "\n",
    "\n",
    "def tokenize_name(name, s_to_i):\n",
    "    token_sequence = [s_to_i[\"<S>\"]]  # special token for start of name\n",
    "    for character in name:\n",
    "        token_sequence.append(s_to_i[character])\n",
    "    token_sequence.append(s_to_i[\"<S>\"])  # special token for end of name\n",
    "    return token_sequence\n",
    "\n",
    "\n",
    "class Embedding:\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        self.E = torch.randn(vocab_size, embedding_dim)\n",
    "        self.E.requires_grad_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.E[x.long()]\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.E]\n",
    "\n",
    "\n",
    "class Flatten:\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        return x.view(B, T * C)\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, input_size, output_size, bias=True):\n",
    "        self.W = torch.randn(input_size, output_size)\n",
    "        self.W.requires_grad_()\n",
    "\n",
    "        if bias:\n",
    "            self.b = torch.randn(output_size)\n",
    "            self.b.requires_grad_()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X @ self.W + self.b\n",
    "\n",
    "    def parameters(self):\n",
    "        params = [self.W]\n",
    "        if hasattr(self, \"b\"):\n",
    "            params.append(self.b)\n",
    "        return params\n",
    "\n",
    "\n",
    "class Sequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params.extend(layer.parameters())\n",
    "        return params\n",
    "\n",
    "\n",
    "class BatchedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "\n",
    "def fit_model(model, X, Y):\n",
    "    ds = BatchedDataset(X, Y)\n",
    "\n",
    "    for _ in range(10000):\n",
    "        for X, Y in torch.utils.data.DataLoader(ds, batch_size=min(32, len(X))):\n",
    "            logits = model.forward(X.float())\n",
    "\n",
    "            loss = torch.nn.functional.cross_entropy(logits, Y)\n",
    "\n",
    "            for param in model.parameters():\n",
    "                param.grad = None\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            for param in model.parameters():\n",
    "                param.data -= 0.1 * param.grad\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def build_dataset(tokenized_names, context_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for tokenized_name in tokenized_names:\n",
    "        # initialize the context to the special token <S> whose id is 0\n",
    "        context = deque([0] * context_length, maxlen=context_length)\n",
    "        for token in tokenized_name[1:]:\n",
    "            y = token\n",
    "            x = list(context)\n",
    "            context.append(token)\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_embeddings(layer, i_to_s):\n",
    "    E = layer.E\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    plt.scatter(E[:, 0], E[:, 1], color=\"blue\", s=800)\n",
    "    for i in range(len(i_to_s)):\n",
    "        ax.text(\n",
    "            E[i, 0],\n",
    "            E[i, 1],\n",
    "            i_to_s[i],\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            color=\"white\",\n",
    "            fontsize=12,\n",
    "        )\n",
    "    return fig\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_proba(model, X):\n",
    "    logits = model(X)\n",
    "    return torch.nn.functional.softmax(logits, dim=1)\n",
    "\n",
    "\n",
    "def sample_name_from_prob_dist(prob_gen, s_to_i, i_to_s, context_length):\n",
    "    # start with an empty name\n",
    "    name = \"\"\n",
    "\n",
    "    # current token is initialized to <S> special token for start of name\n",
    "    context = deque([s_to_i[\"<S>\"]] * context_length, maxlen=context_length)\n",
    "    token = list(context)\n",
    "    while True:\n",
    "        # return the probability of next token given the current token\n",
    "        token_tensor = torch.tensor([token])\n",
    "        prob_next_token = prob_gen(token_tensor)\n",
    "\n",
    "        # given a probability distribution return an integer token sampled from it\n",
    "        next_token = torch.multinomial(input=prob_next_token, num_samples=1).item()\n",
    "\n",
    "        # convert the integer token to a character\n",
    "        next_char = i_to_s[next_token]\n",
    "\n",
    "        # if we have reached the <S> special token for end of name, stop\n",
    "        if next_char == \"<S>\":\n",
    "            break\n",
    "\n",
    "        # update the name\n",
    "        name += next_char\n",
    "\n",
    "        # update the current token\n",
    "        context.append(next_token)\n",
    "        token = list(context)\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "def mlp_with_embedding(names_str, context_length=\"3\", n_names_to_sample=10, seed=0):\n",
    "    # -------------------- Preprocessing arguments --------------------\n",
    "    context_length = int(context_length)\n",
    "    embedding_size = 2\n",
    "    names = names_str.split(\"\\n\")\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # -------------------- Character-Level Tokenization --------------------\n",
    "    s_to_i, i_to_s = character_tokenizer(\"\".join(names))\n",
    "    vocab_size = len(s_to_i)\n",
    "\n",
    "    # -------------------- Tokenized Input --------------------\n",
    "    tokenized_names = [tokenize_name(name, s_to_i) for name in names]\n",
    "\n",
    "    # -------------------- Model input (X) and targets (Y) --------------------\n",
    "    xs, ys = build_dataset(tokenized_names, context_length)\n",
    "\n",
    "    X = torch.tensor(xs)\n",
    "    Y = torch.tensor(ys)\n",
    "\n",
    "    # -------------------- Training --------------------\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Embedding(vocab_size, embedding_size),\n",
    "            Flatten(),\n",
    "            Linear(context_length * embedding_size, vocab_size),\n",
    "        ]\n",
    "    )\n",
    "    loss = fit_model(model, X, Y)\n",
    "\n",
    "    #  -------------------- Learned Embedding --------------------\n",
    "    fig_embed = plot_embeddings(\n",
    "        next(layer for layer in model.layers if isinstance(layer, Embedding)), i_to_s\n",
    "    )\n",
    "\n",
    "    # -------------------- Sampled Output --------------------\n",
    "    sampled_names = []\n",
    "    for _ in range(n_names_to_sample):\n",
    "        name = sample_name_from_prob_dist(\n",
    "            lambda tokens: predict_proba(model, tokens.float()),\n",
    "            s_to_i,\n",
    "            i_to_s,\n",
    "            context_length,\n",
    "        )\n",
    "        sampled_names.append(name)\n",
    "\n",
    "    # -------------------- Random Sampled Output --------------------\n",
    "    prob_uniform = torch.ones((1, vocab_size))\n",
    "    prob_uniform /= prob_uniform.sum(dim=1, keepdim=True)\n",
    "    random_sampled_names = []\n",
    "    for _ in range(n_names_to_sample):\n",
    "        name = sample_name_from_prob_dist(\n",
    "            lambda tokens: prob_uniform, s_to_i, i_to_s, 1\n",
    "        )\n",
    "        random_sampled_names.append(name)\n",
    "\n",
    "    return (\n",
    "        s_to_i,\n",
    "        \"\\n\".join(\n",
    "            [\n",
    "                f\"{name} → {tokenized_name}\"\n",
    "                for name, tokenized_name in zip(names, tokenized_names)\n",
    "            ]\n",
    "        ),\n",
    "        \"\\n\".join([str(x) for x in xs]),\n",
    "        ys,\n",
    "        fig_embed,\n",
    "        f\"{loss:.4f}\",\n",
    "        \"\\n\".join(sampled_names),\n",
    "        \"\\n\".join(random_sampled_names),\n",
    "    )\n",
    "\n",
    "\n",
    "names = [\n",
    "    \"emma\",\n",
    "    \"ava\",\n",
    "]\n",
    "# names = Path(\"names.txt\").read_text().splitlines()\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            title = gr.Markdown(\n",
    "                \"\"\"\n",
    "                # Multi-Layer Perceptron with Embedding\n",
    "                \"\"\"\n",
    "            )\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            text1 = gr.Textbox(label=\"Names Dataset\", value=\"\\n\".join(names))\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            text2 = gr.Textbox(label=\"Context Length\", value=3)\n",
    "\n",
    "    with gr.Row():\n",
    "        inbtw = gr.Button(\"Submit\", variant=\"primary\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out1 = gr.Textbox(label=\"Character-level Tokenization\")\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out2 = gr.Textbox(label=\"Tokenized Input\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out3 = gr.Textbox(label=\"Model input (X)\")\n",
    "            out4 = gr.Textbox(label=\"Model targets (Y)\")\n",
    "\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out5 = gr.Plot(label=\"Learned Embedding\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            out6 = gr.Textbox(label=\"Training Loss (mean negative log-likelihood)\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out7 = gr.Textbox(label=\"Sampled Output\")\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out8 = gr.Textbox(label=\"Random Sampled Output\")\n",
    "\n",
    "    inbtw.click(\n",
    "        mlp_with_embedding,\n",
    "        inputs=[text1, text2],\n",
    "        outputs=[out1, out2, out3, out4, out5, out6, out7, out8],\n",
    "    )\n",
    "\n",
    "demo.launch(quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT model - Attention is all you need\n",
    "\n",
    "A transformer model that uses the attention mechanism to learn contextual relations between words in a text. \n",
    "\n",
    "Attention function: $ \\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}}) V = \\text{weights} V $\n",
    "\n",
    "**Explanation**:\n",
    "For a given token, we perform three linear transformations to obtain:\n",
    "\n",
    "- the query vector Q\n",
    "- the key vector K \n",
    "- the value vector V\n",
    "\n",
    "The matrix multiplication of $QK^T$ scaled by $\\sqrt{d_k}$ and passed through a softmax function can be interpreted as a weighted matrix of the importance of each token in the sequence with respect to the given token. \n",
    "\n",
    "The weighted matrix is then multiplied by the value vector V to obtain the contextualized representation of the given token.\n",
    "\n",
    "As a first step - let's wrap our minds around how a weighted aggregation can be expressed as a matrix multiplication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted aggregations as a matrix multiplication - a mathematical trick in self-attention\n",
    "\n",
    "This shows how we can perform weighted aggregations using a matrix multiplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7880\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7880/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "def display_tensor(t):\n",
    "    return \"\\n\".join(\n",
    "        [\"   \".join([f\"{elem:.2f}\" for elem in row]) for row in t.tolist()]\n",
    "    )\n",
    "\n",
    "\n",
    "def aggergate_via_matrix_mult(sequence_length=3, embedding_size=2, seed=0):\n",
    "    # show intended averaging\n",
    "    \"\"\"\n",
    "    B = batch_size\n",
    "    T = sequence_length # (time steps)\n",
    "    C = embedding_size # (channels)\n",
    "\n",
    "    for b in range(B):\n",
    "        for t in range(T):\n",
    "            # select the batch and keep the first t tokens\n",
    "            xprev = x[b, : t + 1]  # t x C\n",
    "            # average over the channels across the tokens\n",
    "            xbow[b, t] = xprev.mean(dim=0)\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    # ---------------------------- Prepare data ----------------------------\n",
    "    sequence_length = int(sequence_length)\n",
    "    embedding_size = int(embedding_size)\n",
    "    X = torch.randint(0, 10, (sequence_length, embedding_size)).float()\n",
    "\n",
    "    # ---------------------------- Weights as Ones matrix ----------------------------\n",
    "    w_ones = torch.ones((sequence_length, sequence_length))\n",
    "    out_ones = w_ones @ X\n",
    "\n",
    "    # ---------------------------- Weights as  Lower triangular matrix ----------------------------\n",
    "    w_tril = torch.tril(torch.ones((sequence_length, sequence_length)))\n",
    "    out_tril = w_tril @ X\n",
    "\n",
    "    # ---------------------------- Weights as  Lower triangular matrix normalized ----------------------------\n",
    "    w_tril_norm = w_tril / w_tril.sum(dim=1, keepdim=True)\n",
    "    out_tril_norm = w_tril_norm @ X\n",
    "\n",
    "    # ---------------------------- Weights as Lower triangular zeros and infities ----------------------------\n",
    "    w_zeroes = torch.zeros((sequence_length, sequence_length))\n",
    "    w_zeroes = w_zeroes.masked_fill(w_tril == 0, float(\"-inf\"))\n",
    "    softmaxed = torch.softmax(w_zeroes, dim=1)\n",
    "\n",
    "    # show how to normalize using a zeros and inf mask + softmax\n",
    "    # a = torch.zeros((sequence_length, sequence_length))\n",
    "    # a = a.masked_fill(a == 0, float(\"-inf\"))\n",
    "    # a = torch.softmax(a, dim=1)\n",
    "    return (\n",
    "        display_tensor(w_ones),\n",
    "        display_tensor(X),\n",
    "        display_tensor(out_ones),\n",
    "        display_tensor(w_tril),\n",
    "        display_tensor(X),\n",
    "        display_tensor(out_tril),\n",
    "        display_tensor(w_tril_norm),\n",
    "        display_tensor(X),\n",
    "        display_tensor(out_tril_norm),\n",
    "        display_tensor(w_zeroes),\n",
    "        display_tensor(softmaxed),\n",
    "    )\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            title = gr.Markdown(\n",
    "                \"\"\"\n",
    "                # Self attention trick: Weigthed-aggregation using matrix multiplication\n",
    "\n",
    "                In a nutshell - We want to replace the following code with a matrix multiplication:\n",
    "\n",
    "                ```python\n",
    "                x = torch.rand((T, C))\n",
    "                x_bag_of_words = torch.rand((T, C))\n",
    "                for t in range(T):\n",
    "                    # keep all past tokens up to t\n",
    "                    xprev = x[: t + 1]  # t x C\n",
    "                    # average over the channels across the past tokens\n",
    "                    x_bag_of_words[t] = xprev.mean(dim=0)\n",
    "                ```\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            text1 = gr.Textbox(\n",
    "                label=\"Sequence Length or Block/Context size (T)\", value=3\n",
    "            )\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            text2 = gr.Textbox(label=\"Channel size or Embedding Size (C)\", value=2)\n",
    "\n",
    "    with gr.Row():\n",
    "        inbtw = gr.Button(\"Submit\", variant=\"primary\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out1 = gr.Textbox(label=\"Weight: Ones matrix (T, T)\")\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out2 = gr.Textbox(label=\"X matrix (T, C)\")\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out3 = gr.Textbox(label=\"Weight @ X (T, C)\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out4 = gr.Textbox(label=\"Weights: lower triangluar matrix (T, T)\")\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out5 = gr.Textbox(label=\"X matrix (T, C)\")\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out6 = gr.Textbox(label=\"Weight @ X (T, C)\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out7 = gr.Textbox(\n",
    "                label=\"Weights: normalized lower triangluar matrix (T, T)\"\n",
    "            )\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out8 = gr.Textbox(label=\"X matrix (T, C)\")\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out9 = gr.Textbox(label=\"Weight @ X (T, C)\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out10 = gr.Textbox(label=\"Weights: lower triangluar zeros and infinities\")\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out11 = gr.Textbox(label=\"Softmaxed weights\")\n",
    "\n",
    "    inbtw.click(\n",
    "        aggergate_via_matrix_mult,\n",
    "        inputs=[text1, text2],\n",
    "        outputs=[out1, out2, out3, out4, out5, out6, out7, out8, out9, out10, out11],\n",
    "    )\n",
    "\n",
    "demo.launch(quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7882\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7882/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import deque\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(torch.nn.Module):\n",
    "    def __init__(self, block_size, num_embed, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = torch.nn.Linear(num_embed, head_size, bias=False)\n",
    "        self.query = torch.nn.Linear(num_embed, head_size, bias=False)\n",
    "        self.value = torch.nn.Linear(num_embed, head_size, bias=False)\n",
    "        self.register_buffer(\n",
    "            \"tril_mask\", torch.tril(torch.ones(block_size, block_size))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        H = self.head_size\n",
    "\n",
    "        # x (B, T, C) @ key (C, H) -> (B, T, C) @ (B, C, H) -> (B, T, H)\n",
    "        K = self.key(x)\n",
    "\n",
    "        # x (B, T, C) @ query (C, H) -> (B, T, C) @ (B, C, H) -> (B, T, H\n",
    "        Q = self.query(x)\n",
    "\n",
    "        # K (B, T, H) @ Q.T (B, H, T) -> (B, T, T)\n",
    "        W = K @ Q.view(B, H, T)\n",
    "        W = W / torch.sqrt(torch.tensor(self.head_size).float())\n",
    "\n",
    "        # W (B, T, T) -- tril_mask (H, H) -> (B, T, T)\n",
    "        W = W.masked_fill(self.tril_mask[:T, :T] == 0, float(\"-inf\"))\n",
    "        W = torch.softmax(W, dim=-1)\n",
    "\n",
    "        # x (B, T, C) @ value (C, H) -> (B, T, C) @ (B, C, H) -> (B, T, H)\n",
    "        V = self.value(x)\n",
    "\n",
    "        # W (B, T, T) @ V (B, T, H) -> (B, T, H)\n",
    "        out = W @ V\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, block_size, num_embed, num_heads):\n",
    "        super().__init__()\n",
    "        assert num_embed % num_heads == 0\n",
    "        self.attention_heads = torch.nn.ModuleList(\n",
    "            [\n",
    "                ScaledDotProductAttention(\n",
    "                    block_size=block_size,\n",
    "                    num_embed=num_embed,\n",
    "                    head_size=num_embed // num_heads,\n",
    "                )\n",
    "                for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # dim = -1 is H/n - when concatenated back together, we want the last dim to be H\n",
    "        return torch.cat([head(x) for head in self.attention_heads], dim=-1)\n",
    "\n",
    "\n",
    "class TransformerBlock(torch.nn.Module):\n",
    "    \"\"\"A Transformer block (with pre-normalization).\"\"\"\n",
    "\n",
    "    def __init__(self, block_size, num_embed, num_heads):\n",
    "        super().__init__()\n",
    "        self.ln_1 = torch.nn.LayerNorm(num_embed)\n",
    "        self.attn = MultiHeadAttention(\n",
    "            block_size=block_size, num_embed=num_embed, num_heads=num_heads\n",
    "        )\n",
    "        self.ln_2 = torch.nn.LayerNorm(num_embed)\n",
    "        self.mlp = torch.nn.ModuleDict(\n",
    "            dict(\n",
    "                c_fc=torch.nn.Linear(num_embed, 4 * num_embed),\n",
    "                c_proj=torch.nn.Linear(4 * num_embed, num_embed),\n",
    "            )\n",
    "        )\n",
    "        m = self.mlp\n",
    "        self.mlpf = lambda x: m.c_proj(\n",
    "            torch.nn.functional.gelu(m.c_fc(x))\n",
    "        )  # MLP forward\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlpf(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerModel(torch.nn.Module):\n",
    "    \"\"\"Transformer Language Model, GPT-2 like.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, num_embed, num_heads, num_layers, block_size):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_embed = num_embed\n",
    "\n",
    "        self.transformer = torch.nn.ModuleDict(\n",
    "            dict(\n",
    "                wte=torch.nn.Embedding(vocab_size, num_embed),\n",
    "                wpe=torch.nn.Embedding(block_size, num_embed),\n",
    "                h=torch.nn.ModuleList(\n",
    "                    [\n",
    "                        TransformerBlock(\n",
    "                            block_size=block_size,\n",
    "                            num_embed=num_embed,\n",
    "                            num_heads=num_heads,\n",
    "                        )\n",
    "                        for _ in range(num_layers)\n",
    "                    ]\n",
    "                ),\n",
    "                ln_f=torch.nn.LayerNorm(num_embed),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.lm_head = torch.nn.Linear(num_embed * block_size, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        B, T = x.shape\n",
    "\n",
    "        # token embeddings of shape (b, t, num_embed)\n",
    "        tok_emb = self.transformer.wte(x)\n",
    "\n",
    "        # position embeddings of shape (t, num_embed)\n",
    "        pos = torch.arange(T, dtype=torch.long, device=device)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "\n",
    "        # add embeddings by broadcast (b, t, num_embed)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        logits = self.lm_head(x.view(B, T * self.num_embed))\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "def character_tokenizer(names_str):\n",
    "    vocab = sorted(list(set(names_str)) + [\"<S>\"])\n",
    "    s_to_i = {s: i for i, s in enumerate(vocab)}\n",
    "    i_to_s = {i: s for i, s in enumerate(vocab)}\n",
    "    return s_to_i, i_to_s\n",
    "\n",
    "\n",
    "def tokenize_name(name, s_to_i):\n",
    "    token_sequence = [s_to_i[\"<S>\"]]  # special token for start of name\n",
    "    for character in name:\n",
    "        token_sequence.append(s_to_i[character])\n",
    "    token_sequence.append(s_to_i[\"<S>\"])  # special token for end of name\n",
    "    return token_sequence\n",
    "\n",
    "\n",
    "def build_dataset(tokenized_names, context_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for tokenized_name in tokenized_names:\n",
    "        # initialize the context to the special token <S> whose id is 0\n",
    "        context = deque([0] * context_length, maxlen=context_length)\n",
    "        for token in tokenized_name[1:]:\n",
    "            y = token\n",
    "            x = list(context)\n",
    "            context.append(token)\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "class BatchedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "\n",
    "def fit_model(model, x, targets):\n",
    "    ds = BatchedDataset(x, targets)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    for _ in range(2000):\n",
    "        for x, targets in torch.utils.data.DataLoader(ds, batch_size=min(64, len(x))):\n",
    "            logits = model(x)\n",
    "            loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def transformer_model(\n",
    "    names,\n",
    "    block_size,\n",
    "    num_embed,\n",
    "    num_heads,\n",
    "    num_layers,\n",
    "):\n",
    "    torch.manual_seed(0)\n",
    "    names = names.split(\"\\n\")\n",
    "    block_size, num_embed, num_heads, num_layers = map(\n",
    "        int, [block_size, num_embed, num_heads, num_layers]\n",
    "    )\n",
    "\n",
    "    s_to_i, i_to_s = character_tokenizer(\"\".join(names))\n",
    "    vocab_size = len(s_to_i)\n",
    "\n",
    "    tokenized_names = [tokenize_name(name, s_to_i) for name in names]\n",
    "\n",
    "    xs, ys = build_dataset(tokenized_names, context_length=block_size)\n",
    "\n",
    "    batch_size = min(64, len(xs))\n",
    "\n",
    "    model = TransformerModel(\n",
    "        block_size=block_size,\n",
    "        vocab_size=vocab_size,\n",
    "        num_embed=num_embed,\n",
    "        num_layers=num_layers,\n",
    "        num_heads=num_heads,\n",
    "    )\n",
    "\n",
    "    model_summary = repr(model)\n",
    "\n",
    "    # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
    "    n_params = sum(p.numel() for p in model.transformer.parameters())\n",
    "\n",
    "    x = torch.tensor(xs)\n",
    "    targets = torch.tensor(ys)\n",
    "    loss = fit_model(model, x, targets)\n",
    "\n",
    "    return (model_summary, n_params, f\"{loss.item():.2f}\")\n",
    "\n",
    "\n",
    "names = [\"emma\", \"ava\"]\n",
    "# names = Path(\"names.txt\").read_text().splitlines()\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            title = gr.Markdown(\n",
    "                \"\"\"\n",
    "                # Transformer Model: Attention is all you need\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            names = gr.Textbox(label=\"Names\", value=\"\\n\".join(names))\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            text1 = gr.Textbox(label=\"Block size (T)\", value=3)\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            text2 = gr.Textbox(label=\"Embedding Size (C)\", value=4)\n",
    "\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            text3 = gr.Textbox(label=\"Number of Heads (N)\", value=2)\n",
    "\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            text4 = gr.Textbox(label=\"Number of Layers (L)\", value=2)\n",
    "\n",
    "    with gr.Row():\n",
    "        inbtw = gr.Button(\"Submit\", variant=\"primary\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            out1 = gr.Textbox(label=\"Model Summary\", max_lines=50)\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            out2 = gr.Textbox(label=\"Number of Parameters\")\n",
    "        with gr.Column():\n",
    "            out3 = gr.Textbox(label=\"Training Loss - Negative mean Log Likelihood\")\n",
    "\n",
    "    inbtw.click(\n",
    "        transformer_model,\n",
    "        inputs=[\n",
    "            names,\n",
    "            text1,\n",
    "            text2,\n",
    "            text3,\n",
    "            text4,\n",
    "        ],\n",
    "        outputs=[out1, out2, out3],\n",
    "    )\n",
    "\n",
    "demo.launch(quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning with Ray Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ray cluster creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7873\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7873/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import yaml\n",
    "import subprocess\n",
    "import gradio as gr\n",
    "\n",
    "cluster_file = \"cluster.yaml\"\n",
    "\n",
    "\n",
    "def create_cluster(cluster_spec):\n",
    "    cluster_spec = yaml.load(cluster_spec, Loader=yaml.SafeLoader)\n",
    "    with open(cluster_file, \"w\") as f:\n",
    "        yaml.dump(cluster_spec, f)\n",
    "\n",
    "    s = subprocess.run(\n",
    "        [\"ray\", \"up\", \"-y\", cluster_file], check=True, capture_output=True\n",
    "    )\n",
    "    captured_stdout = s.stdout.decode(\"utf-8\")\n",
    "    # Remove ANSI color codes from captured stdout\n",
    "    ansi_escape = re.compile(r\"\\x1B\\[[0-?]*[ -/]*[@-~]\")\n",
    "    stripped_stdout = ansi_escape.sub(\"\", captured_stdout)\n",
    "    return stripped_stdout\n",
    "\n",
    "\n",
    "cluster_spec = {\n",
    "    \"cluster_name\": \"ray-tune-cluster\",\n",
    "    \"provider\": {\n",
    "        \"type\": \"aws\",\n",
    "        \"region\": \"us-west-2\",\n",
    "    },\n",
    "    \"auth\": {\n",
    "        \"ssh_user\": \"ubuntu\",\n",
    "    },\n",
    "    \"min_workers\": 0,\n",
    "    \"max_workers\": 3,\n",
    "    \"available_node_types\": {\n",
    "        \"head_node\": {\n",
    "            \"node_config\": {\n",
    "                \"InstanceType\": \"c5.xlarge\",\n",
    "                \"ImageId\": \"ami-032a22dd6280fbf04\",\n",
    "            }\n",
    "        },\n",
    "        \"worker_nodes\": {\n",
    "            \"node_config\": {\n",
    "                \"InstanceType\": \"c5.xlarge\",\n",
    "                \"ImageId\": \"ami-032a22dd6280fbf04\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    \"head_node_type\": \"head_node\",\n",
    "    \"setup_commands\": [\n",
    "        \"sudo apt-get update\",\n",
    "        \"sudo apt-get install python-is-python3\",\n",
    "        \"pip3 install pip --upgrade\",\n",
    "        \"pip3 install ray[air] torch pandas tensorboard\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            title = gr.Markdown(\n",
    "                \"\"\"\n",
    "                # Ray Cluster Setup on AWS (EC2)\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            input_ = gr.Textbox(\n",
    "                label=\"Cluster Specification\",\n",
    "                value=yaml.dump(cluster_spec, sort_keys=False),\n",
    "                max_lines=30,\n",
    "            )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            inbtw = gr.Button(\"Create Cluster\", variant=\"primary\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            output = gr.Textbox(label=\"Logs\", value=\"\")\n",
    "\n",
    "    inbtw.click(\n",
    "        create_cluster,\n",
    "        inputs=[input_],\n",
    "        outputs=[output],\n",
    "    )\n",
    "\n",
    "demo.launch(quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting simple_tuner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile simple_tuner.py\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import ray\n",
    "from ray import train, tune\n",
    "from ray.tune.tuner import Tuner, TuneConfig\n",
    "from ray.air import session, Checkpoint\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.air.config import ScalingConfig\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, layer_size, output_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_size, layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(layer_size, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.layer2(self.relu(self.layer1(input)))\n",
    "\n",
    "\n",
    "def train_loop_per_worker(config):\n",
    "    dataset_shard = session.get_dataset_shard(\"train\")\n",
    "    model = NeuralNetwork(\n",
    "        input_size=config[\"input_size\"],\n",
    "        layer_size=config[\"layer_size\"],\n",
    "        output_size=config[\"output_size\"],\n",
    "    )\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    model = train.torch.prepare_model(model)\n",
    "\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        for batches in dataset_shard.iter_torch_batches(\n",
    "            batch_size=32, dtypes=torch.float\n",
    "        ):\n",
    "            inputs, labels = torch.unsqueeze(batches[\"x\"], 1), batches[\"y\"]\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        session.report(\n",
    "            {\"loss\": loss.item()},\n",
    "            # note checkpointing requires s3 storage path\n",
    "            # checkpoint=Checkpoint.from_dict(\n",
    "            #     dict(epoch=epoch, model=model.state_dict())\n",
    "            # ),\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--address\")\n",
    "    args = parser.parse_args()\n",
    "    ray.init(address=args.address)\n",
    "\n",
    "    train_dataset = ray.data.from_items([{\"x\": x, \"y\": 2 * x + 1} for x in range(200)])\n",
    "    scaling_config = ScalingConfig(num_workers=3, use_gpu=False)\n",
    "    \n",
    "    # Trainer with default config\n",
    "    trainer = TorchTrainer(\n",
    "        train_loop_per_worker=train_loop_per_worker,\n",
    "        train_loop_config={\n",
    "            \"input_size\": 1,\n",
    "            \"layer_size\": 15,\n",
    "            \"output_size\": 1,\n",
    "            \"num_epochs\": 3,\n",
    "            \"lr\": 1e-3,\n",
    "        },\n",
    "        scaling_config=scaling_config,\n",
    "        datasets={\"train\": train_dataset},\n",
    "    )\n",
    "\n",
    "    # Define the search space.\n",
    "    param_space = {\"train_loop_config\": {\"lr\": tune.loguniform(0.0001, 0.01)}}\n",
    "\n",
    "    tuner = Tuner(\n",
    "        trainer,\n",
    "        param_space=param_space,\n",
    "        tune_config=TuneConfig(num_samples=5, metric=\"loss\", mode=\"min\"),\n",
    "    )\n",
    "\n",
    "    # Execute tuning.\n",
    "    result_grid = tuner.fit()\n",
    "\n",
    "    # Fetch the best result.\n",
    "    best_result = result_grid.get_best_result()\n",
    "    print(\"Best Result:\", best_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-21 12:09:14,768\tINFO util.py:375 -- setting max workers for head node type to 0\n",
      "2023-08-21 12:09:14,768\tINFO util.py:379 -- setting max workers for worker_nodes to 3\n",
      "\u001b[33mLoaded cached provider configuration\u001b[39m\n",
      "\u001b[33mIf you experience issues with the cloud provider, try re-running the command with \u001b[1m--no-config-cache\u001b[22m\u001b[26m.\u001b[39m\n",
      "Destroying cluster. \u001b[4mConfirm [y/N]:\u001b[24m y \u001b[2m[automatic, due to --yes]\u001b[22m\n",
      "2023-08-21 12:09:14,847\tINFO util.py:375 -- setting max workers for head node type to 0\n",
      "2023-08-21 12:09:14,847\tINFO util.py:379 -- setting max workers for worker_nodes to 3\n",
      "\u001b[37mFetched IP\u001b[39m: \u001b[1m35.84.181.167\u001b[22m\n",
      "\u001b[33mStopped only 7 out of 8 Ray processes within the grace period 16 seconds. Set `\u001b[1m-v\u001b[22m\u001b[33m` to see more details. Remaining processes [psutil.Process(pid=9587, name='gcs_server', status='terminated', started='16:01:01')] will be forcefully terminated.\u001b[39m\n",
      "\u001b[33mYou can also use `\u001b[1m--force\u001b[22m\u001b[33m` to forcefully terminate processes or set higher `--grace-period` to wait longer time for proper termination.\u001b[39m\n",
      "\u001b[0mShared connection to 35.84.181.167 closed.\n",
      "Stopping instances \u001b[1mi-09e5f7de5ee010795\u001b[22m\u001b[0m, \u001b[0m\u001b[1mi-0511a8ccd25bae133\u001b[22m\u001b[0m, \u001b[0m\u001b[1mi-0be7f2bc2d12c4f1e\u001b[22m\u001b[0m, \u001b[0m\u001b[1mi-0476f9c83159ce0e6\u001b[22m \u001b[2m(to terminate instead, set `cache_stopped_nodes: False` under `provider` in the cluster configuration)\u001b[22m\n",
      "Requested \u001b[1m4\u001b[22m\u001b[26m nodes to shut down.\u001b[0m\u001b[2m [interval=1s]\u001b[22m\u001b[0m\n",
      "\u001b[1m0\u001b[22m\u001b[26m nodes remaining after 5 second(s).\n",
      "\u001b[32mNo nodes remaining.\u001b[39m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ray down -y cluster.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7884\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7884/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import AutoTokenizer\n",
    "from string import ascii_lowercase, ascii_uppercase, punctuation, digits, whitespace\n",
    "\n",
    "\n",
    "def tokenize_input(input_text=\"Hello tokenization!\"):\n",
    "    # -------------- Character tokenization ---------------\n",
    "    vocab = ascii_lowercase + ascii_uppercase + punctuation + digits + whitespace\n",
    "    char_tokens = [x for x in input_text]\n",
    "    char_token_ids = [vocab.index(x) for x in char_tokens]\n",
    "    char_vocab_size = len(vocab)\n",
    "    char_seq_len = len(char_tokens)\n",
    "\n",
    "    # --------------- Sub-word tokenization ---------------\n",
    "    gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    gpt2_tokens = gpt2_tokenizer.tokenize(input_text)\n",
    "    gpt2_token_ids = gpt2_tokenizer.convert_tokens_to_ids(gpt2_tokens)\n",
    "    gpt2_vocab_size = gpt2_tokenizer.vocab_size\n",
    "    gpt2_seq_len = len(gpt2_tokens)\n",
    "\n",
    "    bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    bert_tokens = bert_tokenizer.tokenize(input_text)\n",
    "    bert_token_ids = bert_tokenizer.convert_tokens_to_ids(bert_tokens)\n",
    "    bert_vocab_size = bert_tokenizer.vocab_size\n",
    "    bert_seq_len = len(bert_tokens)\n",
    "\n",
    "    return (\n",
    "        char_tokens,\n",
    "        gpt2_tokens,\n",
    "        bert_tokens,\n",
    "        char_token_ids,\n",
    "        gpt2_token_ids,\n",
    "        bert_token_ids,\n",
    "        char_vocab_size,\n",
    "        f\"{gpt2_vocab_size:,}\",\n",
    "        f\"{bert_vocab_size:,}\",\n",
    "        char_seq_len,\n",
    "        gpt2_seq_len,\n",
    "        bert_seq_len,\n",
    "    )\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            text1 = gr.Textbox(label=\"Text\", value=\"hello tokenization!\")\n",
    "            inbtw = gr.Button(\"Tokenize\", variant=\"primary\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out1 = gr.Textbox(label=\"Character level Tokens\")\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out2 = gr.Textbox(label=\"Sub-word level Tokens (BPE/GPT2)\")\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out3 = gr.Textbox(label=\"Sub-word level Tokens (SentencePiece/BERT)\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out4 = gr.Textbox(label=\"Character level Token Ids\")\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out5 = gr.Textbox(label=\"Sub-word level Token Ids (BPE/GPT2)\")\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out6 = gr.Textbox(label=\"Sub-word level Token Ids (SentencePiece/BERT)\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out7 = gr.Textbox(label=\"Character level Tokenizer Vocab Size\")\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out8 = gr.Textbox(label=\"Sub-word level Tokenizer Vocab Size (BPE/GPT2)\")\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out9 = gr.Textbox(\n",
    "                label=\"Sub-word level Tokenizer Vocab Size (SentencePiece/BERT)\"\n",
    "            )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out10 = gr.Textbox(label=\"Character level Tokenizer Sequence Length\")\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out11 = gr.Textbox(\n",
    "                label=\"Sub-word level Tokenizer Sequence Length (BPE/GPT2)\"\n",
    "            )\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out12 = gr.Textbox(\n",
    "                label=\"Sub-word level Tokenizer Sequence Length (SentencePiece/BERT)\"\n",
    "            )\n",
    "\n",
    "    inbtw.click(\n",
    "        tokenize_input,\n",
    "        inputs=text1,\n",
    "        outputs=[\n",
    "            out1,\n",
    "            out2,\n",
    "            out3,\n",
    "            out4,\n",
    "            out5,\n",
    "            out6,\n",
    "            out7,\n",
    "            out8,\n",
    "            out9,\n",
    "            out10,\n",
    "            out11,\n",
    "            out12,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "demo.launch(quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ansycale-present-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
